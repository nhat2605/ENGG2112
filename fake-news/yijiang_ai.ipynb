{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "# Same coding step to using one data\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df['author'].fillna('Unknown', inplace=True)\n",
    "df['title'].fillna('Ambiguous', inplace=True)\n",
    "df['text'].fillna('Ambiguous', inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# TF-IDF Vectorization for text and title\n",
    "vectorizer_text = TfidfVectorizer(max_features=5000)\n",
    "X_text = vectorizer_text.fit_transform(df['text']).toarray()\n",
    "\n",
    "vectorizer_title = TfidfVectorizer(max_features=1000)\n",
    "X_title = vectorizer_title.fit_transform(df['title']).toarray()\n",
    "\n",
    "# One-hot encoding for authors\n",
    "encoder = OneHotEncoder()\n",
    "X_author = encoder.fit_transform(df[['author']]).toarray()\n",
    "\n",
    "y = df['label'].values\n",
    "\n",
    "\n",
    "\n",
    "# Combine the feature matrices\n",
    "x = np.hstack((X_text, X_title, X_author))\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(x_train, y_train)\n",
    "y_pre = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0  0.9940973930 0.9479362101 0.9704681873      2132\n",
      "           1  0.9478138223 0.9940828402 0.9703971119      2028\n",
      "\n",
      "    accuracy                      0.9704326923      4160\n",
      "   macro avg  0.9709556077 0.9710095252 0.9704326496      4160\n",
      "weighted avg  0.9715341523 0.9704326923 0.9704335380      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pre, digits=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACC AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is: [0.90625    0.92668269 0.95552885 0.91346154 0.95913462]\n",
      "The testing auc is: [0.98306506 0.98414915 0.99247786 0.98591378 0.99239599]\n"
     ]
    }
   ],
   "source": [
    "# Score and test:\n",
    "test_acc = sklearn.model_selection.cross_val_score(knn, x_test, y_test, scoring = \"accuracy\")\n",
    "print(\"The test accuracy is:\", test_acc)\n",
    "test_auc = sklearn.model_selection.cross_val_score(knn, x_test, y_test, scoring = \"roc_auc\")\n",
    "print(\"The testing auc is:\", test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test one sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of 0 0.0\n",
      "probability of 1 1.0\n",
      "true value 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\12462\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "sample = {\n",
    "    'text': df['text'][0],\n",
    "    'title': df['title'][0],\n",
    "    'author': df['author'][0],\n",
    "    'lable': df['label'][0]\n",
    "}\n",
    "\n",
    "# TF-IDF Vectorization for the new example\n",
    "new_text = vectorizer_text.transform([sample['text']]).toarray()\n",
    "new_title = vectorizer_title.transform([sample['title']]).toarray()\n",
    "new_author = encoder.transform([[sample['author']]]).toarray()\n",
    "\n",
    "# Combine the feature matrices for the new example\n",
    "sample_x = np.hstack((new_text, new_title, new_author))\n",
    "\n",
    "# se the trained KNN model to make a prediction\n",
    "pre = knn.predict_proba(sample_x)\n",
    "print('probability of 0', pre[0][0])\n",
    "print('probability of 1', pre[0][1])\n",
    "print('true value', sample['lable'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def predict_all_articles(df, model, vectorizer_text, vectorizer_title, encoder):\n",
    "    df['author'].fillna('Unknown', inplace=True)\n",
    "    df['title'].fillna('Ambiguous', inplace=True)\n",
    "    df['text'].fillna('Ambiguous', inplace=True)\n",
    "    \n",
    "    # Transform title and text for all articles\n",
    "    X_title = vectorizer_title.transform(df['title']).toarray()\n",
    "    X_text = vectorizer_text.transform(df['text']).toarray()\n",
    "\n",
    "    # Transform the authors\n",
    "    try:\n",
    "        X_author = encoder.transform(df[['author']]).toarray()\n",
    "    except:\n",
    "        # If the author is not recognized from the training data\n",
    "        X_author = np.zeros((df.shape[0], len(encoder.categories_[0])))\n",
    "\n",
    "    #combine\n",
    "    x = np.hstack((X_title, X_text, X_author))\n",
    "    \n",
    "    # Make batch predictions\n",
    "    prediction_probs = model.predict(x)\n",
    "    \n",
    "    # Round the probabilities to get binary class labels\n",
    "    predictions = np.round(prediction_probs).flatten().astype(int)\n",
    "    \n",
    "    # Map 0 and 1 to \"Real News\" and \"Fake News\"\n",
    "    prediction_labels = [\"Real News\" if p == 0 else \"Fake News\" for p in predictions]\n",
    "    \n",
    "    # Add a new column to the original DataFrame to store predictions\n",
    "    df['Prediction'] = prediction_labels\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_accuracy(df):\n",
    "    # Map \"Real News\" and \"Fake News\" back to 0 and 1\n",
    "    df['PredictionLabel'] = df['Prediction'].map({\"Real News\": 0, \"Fake News\": 1})\n",
    "    \n",
    "    # Calculate the number of correct predictions\n",
    "    correct_predictions = df[df['label'] == df['PredictionLabel']].shape[0]\n",
    "    \n",
    "    # Calculate the total number of predictions\n",
    "    total_predictions = df.shape[0]\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    accuracy = (correct_predictions / total_predictions) * 100\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 54.98076923076923%\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_submit = pd.read_csv(\"submit.csv\")\n",
    "# Combine test and submit datasets based on 'id'\n",
    "df_test = pd.merge(df_test, df_submit, on='id')\n",
    "\n",
    "df_with_predictions = predict_all_articles(df_test, knn, vectorizer_text, vectorizer_title, encoder)\n",
    "get_accuracy(df_with_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darrell Lucus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\12462\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\12462\\Desktop\\note\\ENGG2112\\as\\ENGG2112\\fake-news\\yijiang_ai.ipynb 单元格 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     X_author \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mtransform([[sample[\u001b[39m'\u001b[39;49m\u001b[39mauthor\u001b[39;49m\u001b[39m'\u001b[39;49m]]])\u001b[39m.\u001b[39mtoarray()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# If the author is not recognized from the training data\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\12462\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\12462\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1016\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1012\u001b[0m warn_on_unknown \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_unknown \u001b[39min\u001b[39;00m {\n\u001b[0;32m   1013\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1014\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minfrequent_if_exist\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1015\u001b[0m }\n\u001b[1;32m-> 1016\u001b[0m X_int, X_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(\n\u001b[0;32m   1017\u001b[0m     X,\n\u001b[0;32m   1018\u001b[0m     handle_unknown\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_unknown,\n\u001b[0;32m   1019\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1020\u001b[0m     warn_on_unknown\u001b[39m=\u001b[39;49mwarn_on_unknown,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[0;32m   1023\u001b[0m n_samples, n_features \u001b[39m=\u001b[39m X_int\u001b[39m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\12462\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:199\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[1;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)\u001b[0m\n\u001b[0;32m    195\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    196\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound unknown categories \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m in column \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m during transform\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(diff, i)\n\u001b[0;32m    198\u001b[0m     )\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m    200\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Found unknown categories ['tom'] in column 0 during transform",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\12462\\Desktop\\note\\ENGG2112\\as\\ENGG2112\\fake-news\\yijiang_ai.ipynb 单元格 10\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#W6sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mprobability of 0\u001b[39m\u001b[39m'\u001b[39m, pre[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#W6sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mprobability of 1\u001b[39m\u001b[39m'\u001b[39m, pre[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m test_real(sample)\n",
      "\u001b[1;32mc:\\Users\\12462\\Desktop\\note\\ENGG2112\\as\\ENGG2112\\fake-news\\yijiang_ai.ipynb 单元格 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#W6sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     X_author \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39mtransform([[sample[\u001b[39m'\u001b[39m\u001b[39mauthor\u001b[39m\u001b[39m'\u001b[39m]]])\u001b[39m.\u001b[39mtoarray()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# If the author is not recognized from the training data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     X_author \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((sample\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39m], \u001b[39mlen\u001b[39m(encoder\u001b[39m.\u001b[39mcategories_[\u001b[39m0\u001b[39m])))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Combine the feature matrices for the new example\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#W6sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m sample_x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack((new_text, new_title, X_author))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test\n",
    "sample = {\n",
    "    'text': \"\"\"The Security Police point to negative reporting about Finland in Russian media and the closure of the Finnish consulate in St. Petersburg as indicators.\n",
    "Antti Pelttari, the head of Finland's Security Police, states that although Russia is currently focused on its operations in Ukraine and reducing its international isolation, the threat from Russian intelligence and influence in Finland has not disappeared.\n",
    "The ongoing war in Ukraine, increasing tensions between Western countries and Russia, and the imposition of more sanctions are likely to escalate Russia's countermeasures against Finland.\n",
    "The Finnish Security Police also assess that the threat from intelligence activities and influence targeting critical infrastructure has increased, particularly in marine infrastructure.\"\"\",\n",
    "    'title': \"Finland sounds alarm: Russia ready to take action\",\n",
    "    'author': \"tom\"\n",
    "}\n",
    "print(df['author'][0])\n",
    "\n",
    "# test for real news\n",
    "def test_real(news_dict):\n",
    "    # TF-IDF Vectorization for the new example\n",
    "    new_text = vectorizer_text.transform([news_dict['text']]).toarray()\n",
    "    new_title = vectorizer_title.transform([news_dict['title']]).toarray()\n",
    "    # new_author = encoder.transform([[sample[\"author\"]]]).toarray()\n",
    "\n",
    "    try:\n",
    "        X_author = encoder.transform([[sample['author']]]).toarray()\n",
    "    except:\n",
    "        # If the author is not recognized from the training data\n",
    "        X_author = np.zeros((sample.shape[0], len(encoder.categories_[0])))\n",
    "\n",
    "    # Combine the feature matrices for the new example\n",
    "    sample_x = np.hstack((new_text, new_title, X_author))\n",
    "\n",
    "    # se the trained KNN model to make a prediction\n",
    "    pre = knn.predict_proba(sample_x)\n",
    "    print('probability of 0', pre[0][0])\n",
    "    print('probability of 1', pre[0][1])\n",
    "\n",
    "test_real(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
