{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "\n",
    "# Same coding step to using one data\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df['author'].fillna('Unknown', inplace=True)\n",
    "df['title'].fillna('Ambiguous', inplace=True)\n",
    "df['text'].fillna('Ambiguous', inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# TF-IDF Vectorization for text and title\n",
    "vectorizer_text = TfidfVectorizer(max_features=5000)\n",
    "X_text = vectorizer_text.fit_transform(df['text']).toarray()\n",
    "\n",
    "vectorizer_title = TfidfVectorizer(max_features=1000)\n",
    "X_title = vectorizer_title.fit_transform(df['title']).toarray()\n",
    "\n",
    "# One-hot encoding for authors\n",
    "encoder = OneHotEncoder()\n",
    "X_author = encoder.fit_transform(df[['author']]).toarray()\n",
    "\n",
    "y = df['label'].values\n",
    "\n",
    "\n",
    "\n",
    "# Combine the feature matrices\n",
    "x = np.hstack((X_text, X_title, X_author))\n",
    "# Train-test split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(x_train, y_train)\n",
    "y_pre = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0  0.9940973930 0.9479362101 0.9704681873      2132\n",
      "           1  0.9478138223 0.9940828402 0.9703971119      2028\n",
      "\n",
      "    accuracy                      0.9704326923      4160\n",
      "   macro avg  0.9709556077 0.9710095252 0.9704326496      4160\n",
      "weighted avg  0.9715341523 0.9704326923 0.9704335380      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pre, digits=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACC AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is: [0.90625    0.92668269 0.95552885 0.91346154 0.95913462]\n",
      "The testing auc is: [0.98306506 0.98414915 0.99247786 0.98591378 0.99239599]\n"
     ]
    }
   ],
   "source": [
    "# Score and test:\n",
    "test_acc = sklearn.model_selection.cross_val_score(knn, x_test, y_test, scoring = \"accuracy\")\n",
    "print(\"The test accuracy is:\", test_acc)\n",
    "test_auc = sklearn.model_selection.cross_val_score(knn, x_test, y_test, scoring = \"roc_auc\")\n",
    "print(\"The testing auc is:\", test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test one sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\12462\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probability of 0 0.0\n",
      "probability of 1 1.0\n",
      "true value 1\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "sample = {\n",
    "    'text': df['text'][0],\n",
    "    'title': df['title'][0],\n",
    "    'author': df['author'][0],\n",
    "    'lable': df['label'][0]\n",
    "}\n",
    "\n",
    "# TF-IDF Vectorization for the new example\n",
    "new_text = vectorizer_text.transform([sample['text']]).toarray()\n",
    "new_title = vectorizer_title.transform([sample['title']]).toarray()\n",
    "new_author = encoder.transform([[sample['author']]]).toarray()\n",
    "\n",
    "# Combine the feature matrices for the new example\n",
    "sample_x = np.hstack((new_text, new_title, new_author))\n",
    "\n",
    "# se the trained KNN model to make a prediction\n",
    "pre = knn.predict_proba(sample_x)\n",
    "print('probability of 0', pre[0][0])\n",
    "print('probability of 1', pre[0][1])\n",
    "print('true value', sample['lable'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Darrell Lucus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\12462\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found unknown categories ['tom'] in column 0 during transform",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\12462\\Desktop\\note\\ENGG2112\\as\\ENGG2112\\fake-news\\yijiang_ai.ipynb 单元格 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mprobability of 0\u001b[39m\u001b[39m'\u001b[39m, pre[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#X23sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mprobability of 1\u001b[39m\u001b[39m'\u001b[39m, pre[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#X23sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m test_real(sample)\n",
      "\u001b[1;32mc:\\Users\\12462\\Desktop\\note\\ENGG2112\\as\\ENGG2112\\fake-news\\yijiang_ai.ipynb 单元格 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m new_text \u001b[39m=\u001b[39m vectorizer_text\u001b[39m.\u001b[39mtransform([news_dict[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]])\u001b[39m.\u001b[39mtoarray()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m new_title \u001b[39m=\u001b[39m vectorizer_title\u001b[39m.\u001b[39mtransform([news_dict[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m]])\u001b[39m.\u001b[39mtoarray()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m new_author \u001b[39m=\u001b[39m encoder\u001b[39m.\u001b[39;49mtransform([[sample[\u001b[39m\"\u001b[39;49m\u001b[39mauthor\u001b[39;49m\u001b[39m\"\u001b[39;49m]]])\u001b[39m.\u001b[39mtoarray()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Combine the feature matrices for the new example\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/12462/Desktop/note/ENGG2112/as/ENGG2112/fake-news/yijiang_ai.ipynb#X23sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m sample_x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack((new_text, new_title, new_author))\n",
      "File \u001b[1;32mc:\\Users\\12462\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\12462\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:1016\u001b[0m, in \u001b[0;36mOneHotEncoder.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1011\u001b[0m \u001b[39m# validation of X happens in _check_X called by _transform\u001b[39;00m\n\u001b[0;32m   1012\u001b[0m warn_on_unknown \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_unknown \u001b[39min\u001b[39;00m {\n\u001b[0;32m   1013\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1014\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minfrequent_if_exist\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1015\u001b[0m }\n\u001b[1;32m-> 1016\u001b[0m X_int, X_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transform(\n\u001b[0;32m   1017\u001b[0m     X,\n\u001b[0;32m   1018\u001b[0m     handle_unknown\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle_unknown,\n\u001b[0;32m   1019\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1020\u001b[0m     warn_on_unknown\u001b[39m=\u001b[39;49mwarn_on_unknown,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[0;32m   1023\u001b[0m n_samples, n_features \u001b[39m=\u001b[39m X_int\u001b[39m.\u001b[39mshape\n\u001b[0;32m   1025\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_drop_idx_after_grouping \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\12462\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:199\u001b[0m, in \u001b[0;36m_BaseEncoder._transform\u001b[1;34m(self, X, handle_unknown, force_all_finite, warn_on_unknown, ignore_category_indices)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mif\u001b[39;00m handle_unknown \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    195\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    196\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound unknown categories \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m in column \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m during transform\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(diff, i)\n\u001b[0;32m    198\u001b[0m     )\n\u001b[1;32m--> 199\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[0;32m    200\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[39mif\u001b[39;00m warn_on_unknown:\n",
      "\u001b[1;31mValueError\u001b[0m: Found unknown categories ['tom'] in column 0 during transform"
     ]
    }
   ],
   "source": [
    "\n",
    "# Test\n",
    "sample = {\n",
    "    'text': \"\"\"The Security Police point to negative reporting about Finland in Russian media and the closure of the Finnish consulate in St. Petersburg as indicators.\n",
    "Antti Pelttari, the head of Finland's Security Police, states that although Russia is currently focused on its operations in Ukraine and reducing its international isolation, the threat from Russian intelligence and influence in Finland has not disappeared.\n",
    "The ongoing war in Ukraine, increasing tensions between Western countries and Russia, and the imposition of more sanctions are likely to escalate Russia's countermeasures against Finland.\n",
    "The Finnish Security Police also assess that the threat from intelligence activities and influence targeting critical infrastructure has increased, particularly in marine infrastructure.\"\"\",\n",
    "    'title': \"Finland sounds alarm: Russia ready to take action\",\n",
    "    'author': \"tom\"\n",
    "}\n",
    "print(df['author'][0])\n",
    "\n",
    "# test for real news\n",
    "def test_real(news_dict):\n",
    "    # TF-IDF Vectorization for the new example\n",
    "    new_text = vectorizer_text.transform([news_dict['text']]).toarray()\n",
    "    new_title = vectorizer_title.transform([news_dict['title']]).toarray()\n",
    "    new_author = encoder.transform([[sample[\"author\"]]]).toarray()\n",
    "\n",
    "    # Combine the feature matrices for the new example\n",
    "    sample_x = np.hstack((new_text, new_title, new_author))\n",
    "\n",
    "    # se the trained KNN model to make a prediction\n",
    "    pre = knn.predict_proba(sample_x)\n",
    "    print('probability of 0', pre[0][0])\n",
    "    print('probability of 1', pre[0][1])\n",
    "\n",
    "test_real(sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
