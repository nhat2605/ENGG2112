{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-20 10:58:25.328559: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Concatenate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "\n",
    "# Suppress scikit-learn and other warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "df['author'].fillna('Unknown', inplace=True)\n",
    "df['title'].fillna('Ambiguous', inplace=True)\n",
    "df['text'].fillna('Ambiguous', inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# TF-IDF Vectorization for text and title\n",
    "vectorizer_text = TfidfVectorizer(max_features=5000,ngram_range=(1, 3))\n",
    "X_text = vectorizer_text.fit_transform(df['text']).toarray()\n",
    "\n",
    "vectorizer_title = TfidfVectorizer(max_features=1000,ngram_range=(1, 3))\n",
    "X_title = vectorizer_title.fit_transform(df['title']).toarray()\n",
    "\n",
    "# One-hot encoding for authors\n",
    "encoder = OneHotEncoder()\n",
    "X_author = encoder.fit_transform(df[['author']]).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "520/520 [==============================] - 12s 19ms/step - loss: 0.0627 - accuracy: 0.9805\n",
      "Epoch 2/40\n",
      "520/520 [==============================] - 12s 23ms/step - loss: 8.1846e-04 - accuracy: 0.9998\n",
      "Epoch 3/40\n",
      "520/520 [==============================] - 11s 21ms/step - loss: 1.8488e-04 - accuracy: 0.9999\n",
      "Epoch 4/40\n",
      "520/520 [==============================] - 9s 17ms/step - loss: 2.5352e-05 - accuracy: 1.0000\n",
      "Epoch 5/40\n",
      "520/520 [==============================] - 9s 17ms/step - loss: 1.0767e-05 - accuracy: 1.0000\n",
      "Epoch 6/40\n",
      "520/520 [==============================] - 8s 16ms/step - loss: 5.7955e-06 - accuracy: 1.0000\n",
      "Epoch 7/40\n",
      "520/520 [==============================] - 8s 15ms/step - loss: 3.0126e-06 - accuracy: 1.0000\n",
      "Epoch 8/40\n",
      "520/520 [==============================] - 8s 15ms/step - loss: 1.7161e-06 - accuracy: 1.0000\n",
      "Epoch 9/40\n",
      "520/520 [==============================] - 7s 14ms/step - loss: 1.0595e-06 - accuracy: 1.0000\n",
      "Epoch 10/40\n",
      "520/520 [==============================] - 7s 14ms/step - loss: 6.9775e-07 - accuracy: 1.0000\n",
      "Epoch 11/40\n",
      "520/520 [==============================] - 7s 14ms/step - loss: 4.7122e-07 - accuracy: 1.0000\n",
      "Epoch 12/40\n",
      "520/520 [==============================] - 6s 12ms/step - loss: 3.2856e-07 - accuracy: 1.0000\n",
      "Epoch 13/40\n",
      "520/520 [==============================] - 7s 13ms/step - loss: 2.3228e-07 - accuracy: 1.0000\n",
      "Epoch 14/40\n",
      "520/520 [==============================] - 7s 13ms/step - loss: 1.6708e-07 - accuracy: 1.0000\n",
      "Epoch 15/40\n",
      "520/520 [==============================] - 7s 14ms/step - loss: 1.2182e-07 - accuracy: 1.0000\n",
      "Epoch 16/40\n",
      "520/520 [==============================] - 7s 13ms/step - loss: 9.0093e-08 - accuracy: 1.0000\n",
      "Epoch 17/40\n",
      "520/520 [==============================] - 7s 13ms/step - loss: 6.6822e-08 - accuracy: 1.0000\n",
      "Epoch 18/40\n",
      "520/520 [==============================] - 7s 13ms/step - loss: 5.0423e-08 - accuracy: 1.0000\n",
      "Epoch 19/40\n",
      "520/520 [==============================] - 6s 12ms/step - loss: 3.8237e-08 - accuracy: 1.0000\n",
      "Epoch 20/40\n",
      "520/520 [==============================] - 10s 19ms/step - loss: 2.9322e-08 - accuracy: 1.0000\n",
      "Epoch 21/40\n",
      "520/520 [==============================] - 7s 13ms/step - loss: 2.2683e-08 - accuracy: 1.0000\n",
      "Epoch 22/40\n",
      "520/520 [==============================] - 6s 12ms/step - loss: 1.7636e-08 - accuracy: 1.0000\n",
      "Epoch 23/40\n",
      "520/520 [==============================] - 7s 13ms/step - loss: 1.3807e-08 - accuracy: 1.0000\n",
      "Epoch 24/40\n",
      "520/520 [==============================] - 7s 13ms/step - loss: 1.0834e-08 - accuracy: 1.0000\n",
      "Epoch 25/40\n",
      "520/520 [==============================] - 6s 12ms/step - loss: 8.5991e-09 - accuracy: 1.0000\n",
      "Epoch 26/40\n",
      "520/520 [==============================] - 6s 12ms/step - loss: 6.8935e-09 - accuracy: 1.0000\n",
      "Epoch 27/40\n",
      "520/520 [==============================] - 7s 14ms/step - loss: 5.5170e-09 - accuracy: 1.0000\n",
      "Epoch 28/40\n",
      "520/520 [==============================] - 7s 14ms/step - loss: 4.4516e-09 - accuracy: 1.0000\n",
      "Epoch 29/40\n",
      "520/520 [==============================] - 7s 14ms/step - loss: 3.6259e-09 - accuracy: 1.0000\n",
      "Epoch 30/40\n",
      "520/520 [==============================] - 7s 14ms/step - loss: 2.9817e-09 - accuracy: 1.0000\n",
      "Epoch 31/40\n",
      "520/520 [==============================] - 7s 14ms/step - loss: 2.4577e-09 - accuracy: 1.0000\n",
      "Epoch 32/40\n",
      "520/520 [==============================] - 7s 13ms/step - loss: 2.0352e-09 - accuracy: 1.0000\n",
      "Epoch 33/40\n",
      "520/520 [==============================] - 7s 13ms/step - loss: 1.7195e-09 - accuracy: 1.0000\n",
      "Epoch 34/40\n",
      "520/520 [==============================] - 7s 13ms/step - loss: 1.4391e-09 - accuracy: 1.0000\n",
      "Epoch 35/40\n",
      "520/520 [==============================] - 7s 13ms/step - loss: 1.2212e-09 - accuracy: 1.0000\n",
      "Epoch 36/40\n",
      "520/520 [==============================] - 6s 12ms/step - loss: 1.0405e-09 - accuracy: 1.0000\n",
      "Epoch 37/40\n",
      "520/520 [==============================] - 7s 14ms/step - loss: 9.0557e-10 - accuracy: 1.0000\n",
      "Epoch 38/40\n",
      "520/520 [==============================] - 7s 13ms/step - loss: 7.8638e-10 - accuracy: 1.0000\n",
      "Epoch 39/40\n",
      "520/520 [==============================] - 7s 14ms/step - loss: 6.7746e-10 - accuracy: 1.0000\n",
      "Epoch 40/40\n",
      "520/520 [==============================] - 7s 13ms/step - loss: 5.9684e-10 - accuracy: 1.0000\n",
      "130/130 [==============================] - 1s 4ms/step - loss: 0.0092 - accuracy: 0.9983\n",
      "Test Accuracy: 0.998317301273346\n",
      "130/130 [==============================] - 1s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2132\n",
      "           1       1.00      1.00      1.00      2028\n",
      "\n",
      "    accuracy                           1.00      4160\n",
      "   macro avg       1.00      1.00      1.00      4160\n",
      "weighted avg       1.00      1.00      1.00      4160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "y = df['label'].values\n",
    "X_train_text, X_test_text, X_train_title, X_test_title, X_train_author, X_test_author, y_train, y_test = train_test_split(\n",
    "    X_text, X_title, X_author, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Neural Network Architecture\n",
    "input_text = Input(shape=(X_train_text.shape[1],))\n",
    "input_title = Input(shape=(X_train_title.shape[1],))\n",
    "input_author = Input(shape=(X_train_author.shape[1],))\n",
    "\n",
    "# Layers for text\n",
    "x1 = Dense(128, activation='relu')(input_text)\n",
    "x1 = Dense(64, activation='relu')(x1)\n",
    "\n",
    "# Layers for title\n",
    "x2 = Dense(128, activation='relu')(input_title)\n",
    "x2 = Dense(64, activation='relu')(x2)\n",
    "\n",
    "# Layers for author\n",
    "x3 = Dense(128, activation='relu')(input_author)\n",
    "x3 = Dense(64, activation='relu')(x3)\n",
    "\n",
    "# Concatenate\n",
    "concat = Concatenate()([x1, x2, x3])\n",
    "\n",
    "# Final layers\n",
    "out = Dense(64, activation='relu')(concat)\n",
    "out = Dense(1, activation='sigmoid')(out)\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[input_text, input_title, input_author], outputs=out)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train model\n",
    "model.fit([X_train_text, X_train_title, X_train_author], y_train, epochs=40, batch_size=32)\n",
    "\n",
    "# Evaluate model\n",
    "score = model.evaluate([X_test_text, X_test_title, X_test_author], y_test)\n",
    "print(f\"Test Accuracy: {score[1]}\")\n",
    "\n",
    "y_pred = model.predict([X_test_text, X_test_title, X_test_author])\n",
    "y_pred = np.round(y_pred).flatten()  # Round the probabilities to get binary class labels\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.998317301273346\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {score[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 356ms/step\n",
      "Fake News\n"
     ]
    }
   ],
   "source": [
    "def predict_article(title, text, author, model, vectorizer_text, vectorizer_title, encoder):\n",
    "    # Transform title and text using the respective TfidfVectorizers\n",
    "    X_title = vectorizer_title.transform([title]).toarray()\n",
    "    X_text = vectorizer_text.transform([text]).toarray()\n",
    "\n",
    "    # Transform the author using OneHotEncoder\n",
    "    try:\n",
    "        X_author = encoder.transform([[author]]).toarray()\n",
    "    except:\n",
    "        # If the author is not recognized from the training data\n",
    "        X_author = np.zeros((1, len(encoder.categories_[0])))\n",
    "\n",
    "    # Predict using the trained model\n",
    "    prediction_prob = model.predict([X_text, X_title, X_author])\n",
    "    \n",
    "    # Return the binary prediction (0 for real news and 1 for fake news)\n",
    "    return int(np.round(prediction_prob)[0][0])\n",
    "\n",
    "# Example usage\n",
    "title = \"Aliens Land in Central Park!\"\n",
    "text = (\"In a surprising turn of events, extraterrestrial beings made contact with Earth by landing their spaceship in New York's Central Park. \"\n",
    "       \"Thousands of onlookers watched in awe as the unidentified creatures emerged, announcing their peaceful intentions. \"\n",
    "       \"Authorities have quarantined the area and are in talks with the visitors.\")\n",
    "author = \"Jacob\"\n",
    "\n",
    "prediction = predict_article(title, text, author, model, vectorizer_text, vectorizer_title, encoder)\n",
    "print(\"Fake News\" if prediction else \"Real News\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def predict_all_articles(df, model, vectorizer_text, vectorizer_title, encoder):\n",
    "    df['author'].fillna('Unknown', inplace=True)\n",
    "    df['title'].fillna('Ambiguous', inplace=True)\n",
    "    df['text'].fillna('Ambiguous', inplace=True)\n",
    "    \n",
    "    # Transform title and text for all articles\n",
    "    X_title = vectorizer_title.transform(df['title']).toarray()\n",
    "    X_text = vectorizer_text.transform(df['text']).toarray()\n",
    "\n",
    "    # Transform the authors\n",
    "    try:\n",
    "        X_author = encoder.transform(df[['author']]).toarray()\n",
    "    except:\n",
    "        # If the author is not recognized from the training data\n",
    "        X_author = np.zeros((df.shape[0], len(encoder.categories_[0])))\n",
    "    \n",
    "    # Make batch predictions\n",
    "    prediction_probs = model.predict([X_text, X_title, X_author])\n",
    "    \n",
    "    # Round the probabilities to get binary class labels\n",
    "    predictions = np.round(prediction_probs).flatten().astype(int)\n",
    "    \n",
    "    # Map 0 and 1 to \"Real News\" and \"Fake News\"\n",
    "    prediction_labels = [\"Real News\" if p == 0 else \"Fake News\" for p in predictions]\n",
    "    \n",
    "    # Add a new column to the original DataFrame to store predictions\n",
    "    df['Prediction'] = prediction_labels\n",
    "    \n",
    "    return df\n",
    "\n",
    "def get_accuracy(df):\n",
    "    # Map \"Real News\" and \"Fake News\" back to 0 and 1\n",
    "    df['PredictionLabel'] = df['Prediction'].map({\"Real News\": 0, \"Fake News\": 1})\n",
    "    \n",
    "    # Calculate the number of correct predictions\n",
    "    correct_predictions = df[df['label'] == df['PredictionLabel']].shape[0]\n",
    "    \n",
    "    # Calculate the total number of predictions\n",
    "    total_predictions = df.shape[0]\n",
    "    \n",
    "    # Calculate the accuracy\n",
    "    accuracy = (correct_predictions / total_predictions) * 100\n",
    "    \n",
    "    print(f\"Accuracy: {accuracy}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 1s 5ms/step\n",
      "Accuracy: 67.75%\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"test.csv\")\n",
    "df_submit = pd.read_csv(\"submit.csv\")\n",
    "# Combine test and submit datasets based on 'id'\n",
    "df_test = pd.merge(df_test, df_submit, on='id')\n",
    "\n",
    "df_with_predictions = predict_all_articles(df_test, model, vectorizer_text, vectorizer_title, encoder)\n",
    "get_accuracy(df_with_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1404/1404 [==============================] - 6s 4ms/step\n",
      "Accuracy: 52.360906944630045%\n"
     ]
    }
   ],
   "source": [
    "df_test_1 = pd.read_csv(\"Combined_modified.csv\")\n",
    "df_with_predictions_1 = predict_all_articles(df_test_1, model, vectorizer_text, vectorizer_title, encoder)\n",
    "get_accuracy(df_with_predictions_1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
